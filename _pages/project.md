---
title:  " "
layout: splash
permalink: /my_docs/project
author_profile: true
comments: true
---
<link rel="stylesheet" href="/assets/styles.css">
<font face="Georgia" size="3" line-height:5 >
<!--------------------------------------------------------------------------------------------------------------------------->

<h3>Seeing to Learn (S2l) : Observation learning in robotics (Oct 2017 – Present)</h3>
<img src="/assets/images/proj_s2l.gif" alt="Seeing to learn" height="700" width="300" align="left" hspace="20" >
<p align="justify">
Seeing to learn (S2l) is an computer vision-robotics project currently ongoing at University of Leeds, which is aimed at developing advanced observation learning methods for robotics systems. It addresses the inability of current robotic systems to learn from human demonstrations. The project envisions a future where robots could acquire new skills by just observing humans perform a task or even by watching online tutorial videos of demonstrations. In future the robots equipped with these learning methods could be applied in several real world conditions ranging from home to work environments such as construction sites where it could learn to perform the relevant tasks of drilling holes, hammering nails or screwing a bolt just by observing other workers.
<a href="https://leopauly.github.io/s2l/">[website]</a>
</p>
<br>
<!--------------------------------------------------------------------------------------------------------------------------->


<h3>Self-Repairing Cities (Oct 2016 – Sep 2017)</h3>
<p align="justify">
'Self Repairing Cities' is a 5-year EPSRC granted £4.2m project aimed at exploring the use of robots and autonomous systems in city infrastructure. The team is formed of a consortium of the Universities of Leeds (lead), Birmingham, UCL and Southampton.<br>
<img src="/assets/images/porj_road.png" alt="Seeing to learn" height="200" width="500" hspace="20" align="left" border="100"/>
My main role in the project was to explore the use of deep learning based computer vision methods for crack detection in pavement images. A special kind of neural networks called convolutional neural networks (CNNs) was used for this. The convolutional layers in the CNN were able to automatically extract relevant visual features from pavement images for detecting cracks by using its multi layered hierarchal network architecture. These features were further used by the fully connected layers in the CNN to effectively classify the input pavement image into cracked and non-cracked categories. The results of this work was published in International conference for Automation and Robotics in Construction (ISARC) 2017.<br>
<a href="http://selfrepairingcities.com/">[website]</a>
<a href="https://core.ac.uk/download/pdf/96765657.pdf"> [view paper] </a>
<a href="https://github.com/leopauly/ISARC-2017"> [view code] </a>
<br>
As a part of the project, I also worked on developing a simple visual tracking system for tracking colour coded autonomous ground vehicle from a drone, as a part of our entry 'A mock-up earthmoving truck autonomous operation from drones' to the 'Robots for Resilient Infrastructure competition' held in Leeds in association with UK robotics week 2017.
<a href="https://github.com/leopauly/Visual-tracking-from-a-Drone"> [view code] </a>
</p>
<br>
<!--------------------------------------------------------------------------------------------------------------------------->


<h3>Computer Vision Based System Drowsiness Detection System (Aug 2015 – May 2016)</h3>
<p>
<div class="container" align="left" hspace="20">
<iframe src="https://www.youtube.com/embed/11ocmhjUZDA" align="left">
</iframe>
</div>
</p>
<p align="justify">
Project descriptionThe aim of the project was to develop a computer vision system for drowsiness detection. The developed system uses a ordinary web camera and uses a computer vision algorithm to detect whether the subject is drowsy or active.The first stage of the developed system is a tracking algorithm that accurately tracks human eyes in the video of the subject taken using a normal consumer grade camera. Then it determines whether the eyes are closed or open using a combination of the HOG features and SVM classifiers. After this the system calculates the PERCLOS (time for which eyes were closed in 1 min) value. So if the value is above a particular threshold then the person is drowsy else is active.
<a href="http://leopauly.blogspot.com/2016/09/computer-vision-system-for-drowsiness.html">[blog]</a>
<a href="https://ieeexplore.ieee.org/abstract/document/7434232">[paper 1]</a>
<a href="https://ieeexplore.ieee.org/abstract/document/7449931">[paper 2]</a>
<a href="http://www.mecs-press.org/ijigsp/ijigsp-v8-n10/IJIGSP-V8-N10-2.pdf">[paper 3]</a>
<a href="https://github.com/leopauly/Drowsiness-Detection-system"> [view code] </a>
</p>
<br>
<!--------------------------------------------------------------------------------------------------------------------------->


<h3>CAMbot : Customer Assistance Mobile Manupulator Robot (Jul 2014 – Apr 2015)</h3>
<img src="/assets/images/proj_cambot.jpg" height="380" width="200"  align="left" hspace="20"/>
<p align="justify">
Project descriptionThe project develops A Customer Assistance Mobile Manipulator Robot that can be used in super markets and big shopping malls .It assists customers by recommending products by facial, gender and emotion recognition using image processing techniques, finding the required products with the help of a user interface developed in C# , identifying commodities in shelves using a RFID tagging system, helping the customers to navigate through the store using buidin maps stored in the robotic memory and helping them to pick and place the commodities using its robotic arm. The project has an interdisciplinary nature and includes Image processing (Robotic vision), embedded system (Microcontrollers), Electrical components (Motors, Servo and drivers), Mechanical design and kinematics (Load- Motor torque relation & robotic arm), Electronic circuits (power supply circuits) and programming (C#, Matlab, python &embedded C).
<a href="https://caskbot.wordpress.com/">[website]</a>
<a href="https://ieeexplore.ieee.org/abstract/document/7456644">[paper 1]</a>
<a href="https://ieeexplore.ieee.org/abstract/document/7475299">[paper 2]</a>
<a href="https://ieeexplore.ieee.org/abstract/document/7489551">[paper 3]</a>
<a href="https://github.com/leopauly/CAMbot-Customer-Assistance-Mobile-Manipulator-Robot">[code]</a>
</p>
<br>
<!--------------------------------------------------------------------------------------------------------------------------->
<br>
<h3>Automatic sorting and grading system for mangos (May 2014 – Dec 2014)</h3>
<div class="container_mango" align="left" hspace="20">
<img style="height=10; width=10;" src="/assets/images/proj_mango.png" height="450" width="500"  align="left"/>
</div>
<p align="justify">
The project was aimed at developing an automatic system for sorting and grading of mangos based on computer vision algorithms using a robotic arm. The application of this system is aimed to replace the existing manual based technique of sorting and grading used in India. The developed system speeds up the entire process considerably and is more accurate and efficient than the traditional techniques used.
The project was funded under Technical Education Improvement Programme (TEQIP) Phase II , CUSAT.<br>
<a href="https://github.com/leopauly/Robotic-Arm-With-Computer-Vision">[code]</a>
<a href="https://ieeexplore.ieee.org/abstract/document/7154891">[paper]</a>
<a href="http://leopauly.blogspot.com/2016/09/mango-images-in-wild-miw-database.html">[dataset]</a>
</p>

<!--------------------------------------------------------------------------------------------------------------------------->
</font>
