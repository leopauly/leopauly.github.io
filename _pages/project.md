---
title:  " "
layout: splash
permalink: /my_docs/project
author_profile: true
comments: true
---
<link rel="stylesheet" href="/assets/styles.css">
<font face="Georgia" size="3" >
<!--------------------------------------------------------------------------------------------------------------------------->

<h3>Seeing to Learn (S2l) : Observational learning of robotic manipulation tasks (Oct 2017 – Sep 2020)</h3>
<img src="/assets/images/proj_s2l.gif" alt="Seeing to learn" height="700" width="300" align="left" hspace="20" >
<p align="justify">
Seeing to learn (S2l) project aims to develop observational learning approaches for robotic systems. It addresses the inability of current systems to learn new tasks solely from third person demonstrations, directly or from online videos. The project envisions a future, where robots equipped with observational learning can aquire new skills and abilities like human beings from demonstrations.
<a href="https://leopauly.github.io/s2l/">[website]</a>
<a href="https://www.frontiersin.org/articles/10.3389/frobt.2021.686368/full"> [paper]</a>
<a href="https://docs.google.com/presentation/d/1pMMhYbRxoYhzjQuysP6tOBuzaTrlubu1/edit?usp=sharing&ouid=101733491474689445191&rtpof=true&sd=true"> [slides]</a>
</p>
<br>
<br>
<!--------------------------------------------------------------------------------------------------------------------------->


<h3>Self-Repairing Cities (Oct 2016 – Sep 2017)</h3>
<p align="justify">
'Self Repairing Cities' is a 5-year EPSRC granted £4.2m project aimed at exploring the use of robots and autonomous systems in city infrastructure. The team is formed of a consortium of the Universities of Leeds (lead), Birmingham, UCL and Southampton.<br>
<img src="/assets/images/porj_road.png" alt="Seeing to learn" height="500" width="530" hspace="20" align="left" border="100"/>
My role in the project was to explore the use of deep learning based computer vision methods for crack detection in pavement images. A special kind of neural networks called convolutional neural networks (CNNs) was used for this. The convolutional layers in the CNN were able to automatically extract relevant visual features from pavement images for detecting cracks by using its multi layered hierarchal network architecture. These features were further used by the fully connected layers in the CNN to effectively classify the input pavement image into cracked and non-cracked categories. The results of this work was published in International conference for Automation and Robotics in Construction (ISARC) 2017.
<a href="http://selfrepairingcities.com/">[website]</a>
<a href="https://core.ac.uk/download/pdf/96765657.pdf"> [view paper] </a>
<a href="https://github.com/leopauly/ISARC-2017"> [view code] </a>
</p>
<br>
<!--------------------------------------------------------------------------------------------------------------------------->

<h3>Computer Vision Based System Drowsiness Detection System (Aug 2015 – May 2016)</h3>
<p>
<div class="container" align="left" hspace="20">
<iframe src="https://www.youtube.com/embed/11ocmhjUZDA" align="left">
</iframe>
</div>
</p>
<p align="justify">
The aim of the project was to develop a computer vision system for drowsiness detection. The developed system uses a ordinary web camera and uses a computer vision algorithm to detect whether the subject is drowsy or active.The first stage of the developed system is a tracking algorithm that accurately tracks human eyes in the video of the subject taken using a normal consumer grade camera. Then it determines whether the eyes are closed or open using a combination of the HOG features and SVM classifiers. After this the system calculates the PERCLOS (time for which eyes were closed in 1 min) value. So if the value is above a particular threshold then the person is drowsy else is active.
<a href="http://leopauly.blogspot.com/2016/09/computer-vision-system-for-drowsiness.html">[blog]</a>
<a href="https://ieeexplore.ieee.org/abstract/document/7434232">[paper 1]</a>
<a href="https://ieeexplore.ieee.org/abstract/document/7449931">[paper 2]</a>
<a href="http://www.mecs-press.org/ijigsp/ijigsp-v8-n10/IJIGSP-V8-N10-2.pdf">[paper 3]</a>
<a href="https://github.com/leopauly/Drowsiness-Detection-system"> [view code] </a>
</p>
<br>
<!--------------------------------------------------------------------------------------------------------------------------->


<h3>CAMbot : Customer Assistance Mobile Manupulator Robot (Jul 2014 – Apr 2015)</h3>
<img src="/assets/images/proj_cambot.jpg" height="380" width="200"  align="left" hspace="20"/>
<p align="justify">
The project develops A Customer Assistance Mobile Manipulator Robot that can be used in super markets and big shopping malls .It assists customers by recommending products by facial, gender and emotion recognition using image processing techniques, finding the required products with the help of a user interface developed in C# , identifying commodities in shelves using a RFID tagging system, helping the customers to navigate through the store using buidin maps stored in the robotic memory and helping them to pick and place the commodities using its robotic arm. The project has an interdisciplinary nature and includes Image processing (Robotic vision), embedded system (Microcontrollers), Electrical components (Motors, Servo and drivers), Mechanical design and kinematics (Load- Motor torque relation & robotic arm), Electronic circuits (power supply circuits) and programming (C#, Matlab, python &embedded C).
<a href="https://caskbot.wordpress.com/">[website]</a>
<a href="https://ieeexplore.ieee.org/abstract/document/7456644">[paper 1]</a>
<a href="https://ieeexplore.ieee.org/abstract/document/7475299">[paper 2]</a>
<a href="https://ieeexplore.ieee.org/abstract/document/7489551">[paper 3]</a>
<a href="https://github.com/leopauly/CAMbot-Customer-Assistance-Mobile-Manipulator-Robot">[code]</a>
</p>
<br>
<!--------------------------------------------------------------------------------------------------------------------------->
<br>
<h3>Automatic sorting and grading system for mangos (May 2014 – Dec 2014)</h3>
<div class="container_mango" align="left" hspace="20">
<img style="height=10; width=10;" src="/assets/images/proj_mango.png" height="450" width="500"  align="left"/>
</div>
<p align="justify">
The project was aimed at developing an automatic system for sorting and grading of mangos based on computer vision algorithms using a robotic arm. The application of this system is aimed to replace the existing manual based technique of sorting and grading used in India. The developed system speeds up the entire process considerably and is more accurate and efficient than the traditional techniques used.
The project was funded under Technical Education Improvement Programme (TEQIP) Phase II , CUSAT.<br>
<a href="https://github.com/leopauly/Robotic-Arm-With-Computer-Vision">[code]</a>
<a href="https://ieeexplore.ieee.org/abstract/document/7154891">[paper]</a>
<a href="http://leopauly.blogspot.com/2016/09/mango-images-in-wild-miw-database.html">[dataset]</a>
</p>

<!--------------------------------------------------------------------------------------------------------------------------->

<br>
<h3>Handwritten Digit Recognition System for South Indian Languages (Sept 2013 - Dec 2014)</h3>
<div class="container_mango" align="left" hspace="20">
<img style="height=5; width=5;" src="/assets/images/charactor.png" height="300" width="300"  align="left"/>
</div>
<p align="justify">
We developed a novel approach for recognition of handwritten digits for South Indian languages using artificial neural networks (ANN) and Histogram of Oriented Gradients (HOG) features. The images of documents containing the hand written digits are optically scanned and are segmented into individual images of isolated digits. HOG features are then extracted from these images and applied to the ANN for recognition. The system recognises the digits with an overall accuracy of 83.4%.
<a href="https://ieeexplore.ieee.org/abstract/document/7346665">[paper]</a>
<br>
</p>

<!--------------------------------------------------------------------------------------------------------------------------->

</font>
